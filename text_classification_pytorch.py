# -*- coding: utf-8 -*-
"""classification pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BuikMC1vzY1NIA3t4DzFINCrlP0w-nM-
"""

## 深度学习，使用Pytorch实现基于RNN、LSTM、BiLSTM和GRU的文本分类

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn import metrics
#torch.set_printoptions(sci_mode=True)

path = '/content/drive/My Drive/data_sampled.csv'

def label_to_category(label):
  if label == '词':
    return 0
  elif label == '诗':
    return 1
  elif label == '文言文':
    return 2
  elif label == '新闻':
    return 3
  elif label == '期刊':
    return 4
  else:
    print("标签格式存在异常")

def get_data(file_path):#获取数据集的文本和标签信息
    data = pd.read_csv(file_path)
    # 取出文本部分
    ci = data[data.类别=='词'].sample(n=1000,random_state=10, axis=0)
    poem = data[data.类别=='诗'].sample(n=1000,random_state=12, axis=0)
    wyw = data[data.类别=='文言文'].sample(n=1000,random_state=15, axis=0)
    news = data[data.类别=='新闻'].sample(n=1000,random_state=16, axis=0)
    journal = data[data.类别=='期刊'].sample(n=1000,random_state=18, axis=0)
    print(len(ci),len(poem),len(wyw),len(news),len(journal))
    # 取出标签部分
    temp = pd.concat([ci,poem,wyw,news,journal])
    temp = temp[['文本','类别']]
    temp.to_csv('/content/drive/My Drive/data_sampled_1000.csv')
    content= data['文本']
    labels = np.asarray([label_to_category(label) for label in data['类别']])
    return content, labels
 
content,labels=get_data(path)

content

labels

word_index = np.load('/content/drive/My Drive/word_index.npy',allow_pickle=True).item()
embeddings_matrix=np.load('/content/drive/My Drive/embedding_matrix.npy')

word_index

embeddings_matrix.shape

#用向量模型处理文本数据
# 序号化 文本，tokenizer句子，并返回每个句子所对应的词语索引
# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理
from keras.preprocessing import sequence
MAX_SEQUENCE_LENGTH=100
def tokenizer(texts, word_index):
    data = []
    for sentence in texts:
        new_sentence = []
        for word in sentence:
            if word!=' ':
                try:
                    new_sentence.append(word_index[word])  # 把文本中的 词语转化为index
                except:
                    new_sentence.append(0)


        data.append(new_sentence)
    # 使用keras的内置函数padding对齐句子,好处是输出numpy数组，不用自己转化了
    data = sequence.pad_sequences(data, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

    return data

X_padded = tokenizer(content, word_index)
del content

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_padded, labels, test_size=0.01, random_state=21) #98:1:1
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=21)

print(len(X_train),len(X_val),len(X_test))

#from keras.utils import to_categorical
#y_train = to_categorical(y_train, num_classes=5)
#y_test = to_categorical(y_test, num_classes=5)
#del X_padded

X_train

y_train

#print(X_train.shape, y_train.shape, X_test.shape,y_test.shape)

import torch.utils.data as Data

x_train_tensor = torch.LongTensor(X_train)
y_train_tensor = torch.LongTensor(y_train)
x_val_tensor = torch.LongTensor(X_val)
y_val_tensor = torch.LongTensor(y_val)
x_test_tensor = torch.LongTensor(X_test)
y_test_tensor = torch.LongTensor(y_test)
train_data = Data.TensorDataset(x_train_tensor, y_train_tensor) # 将数据放入 torch_dataset
val_data = Data.TensorDataset(x_val_tensor, y_val_tensor)
test_data = Data.TensorDataset(x_test_tensor, y_test_tensor)
BATCH_SIZE = 128
train_loader=Data.DataLoader(
        dataset=train_data,           # 将数据放入loader
        batch_size=BATCH_SIZE,           # 每个数据段大小为  BATCH_SIZE=5
        shuffle=True ,                   # 是否打乱数据的排布
        )
val_loader=Data.DataLoader(
        dataset=val_data,           # 将数据放入loader
        batch_size=BATCH_SIZE,           # 每个数据段大小为  BATCH_SIZE=5
        shuffle=True ,                   # 是否打乱数据的排布
        )
test_loader=Data.DataLoader(
        dataset=test_data,           # 将数据放入loader
        batch_size=BATCH_SIZE,           # 每个数据段大小为  BATCH_SIZE=5
        shuffle=True ,                   # 是否打乱数据的排布
        )
def go(loader):
  for step, (x,y) in enumerate(loader):
    print(x)
    print(y)
    break
go(train_loader)
go(val_loader)
go(test_loader)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(device)

def create_emb_layer(weights_matrix, non_trainable=False):
    num_embeddings, embedding_dim = weights_matrix.shape
    emb_layer = nn.Embedding.from_pretrained(weights_matrix)
    emb_layer.load_state_dict({'weight': weights_matrix})
    if non_trainable:
        emb_layer.weight.requires_grad = False

    return emb_layer, num_embeddings, embedding_dim

class RNN_MODEL(nn.Module):
    def __init__(self, weights_matrix, hidden_size, num_layers):
        super(RNN_MODEL,self).__init__()
        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)
        #self.drop = nn.Dropout(p=0.25)
        self.fc = nn.Linear(hidden_size, 5)

    def forward(self, x):        
        out = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]
        #print(out.shape)
        out, _ = self.rnn(out)
        #print(out.shape)
        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state
        #print(out.shape)
        out = F.softmax(out, dim=1)
        return out
    
    def init_hidden(self, batch_size):
        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))

#这个是词向量矩阵的名字
weights_matrix = torch.Tensor(embeddings_matrix)
#print(weights_matrix)


hidden_size = 120 # 隐藏神经元数量
num_layers = 1 # 搭1层
num_epochs = 20 #训练20次
learning_rate = 1e-3 

#初始化模型
rnn_model = RNN_MODEL(weights_matrix,hidden_size, num_layers)
rnn_model.train()

# Training Function

def train(model,optimizer,criterion = nn.CrossEntropyLoss(), train_loader = train_loader, valid_loader = val_loader,
                                             num_epochs = 10,
                                             eval_every = len(train_loader) // 2,
                                             best_valid_loss = float("Inf")
                                             ):
      
    # initialize running values
    running_loss = 0.0
    running_acc = 0.0
    valid_running_loss = 0.0
    valid_running_acc = 0.0
    global_step = 0
    train_loss_list = []
    valid_loss_list = []
    global_steps_list = []

    # training loop
    model.train()
    for epoch in range(num_epochs):
        for i, (trains, labels) in enumerate(train_loader):   
            labels = labels.to(device)
            trains = trains.to(device)
            output = model(trains)

            loss = criterion(output, labels)
            pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
            accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # update running values
            running_loss += loss.item()
            running_acc += accuracy
            global_step += 1

            # evaluation step
            if global_step % eval_every == 0:
                model.eval()
                with torch.no_grad():                    
                  # validation loop
                  for i, (vals, labels) in enumerate(valid_loader):
                      labels = labels.to(device)
                      vals = vals.to(device)
                      output = model(vals)
                      pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
                      accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))
                      loss = criterion(output, labels)
                      valid_running_loss += loss.item()
                      valid_running_acc += accuracy

                # evaluation
                average_train_loss = running_loss / eval_every
                average_train_acc = running_acc / eval_every
                average_valid_loss = valid_running_loss / len(valid_loader)
                average_valid_acc = valid_running_acc / len(valid_loader)
                train_loss_list.append(average_train_loss)
                valid_loss_list.append(average_valid_loss)
                global_steps_list.append(global_step)

                # resetting running values
                running_loss = 0.0     
                running_acc = 0.0           
                valid_running_loss = 0.0
                valid_running_acc = 0.0
                model.train()

                # print progress
                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Valid Loss: {:.4f}, Valid Acc: {:.4f}'
                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),
                              average_train_loss, average_train_acc, average_valid_loss,average_valid_acc))
                
                # checkpoint
                if best_valid_loss > average_valid_loss:
                    best_valid_loss = average_valid_loss
                    #save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)
                    #save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    
    #save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    print('Finished Training!')

rnn_model = rnn_model.to(device)
optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)
train(model=rnn_model, optimizer=optimizer, num_epochs=20)

class LSTM_MODEL(nn.Module):
    def __init__(self, weights_matrix, hidden_size, num_layers):
        super(LSTM_MODEL,self).__init__()
        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)
        self.drop = nn.Dropout(p=0.25)
        #self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 5)

    def forward(self, x):        
        out = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]
        #print(out.shape)
        out, _ = self.lstm(out)
        #print(out.shape)
        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state
        #print(out.shape)
        out = F.softmax(out, dim=1)
        return out
    
    def init_hidden(self, batch_size):
        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))

def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):

    if save_path == None:
        return
    
    state_dict = {'train_loss_list': train_loss_list,
                  'valid_loss_list': valid_loss_list,
                  'global_steps_list': global_steps_list}
    
    torch.save(state_dict, save_path)
    print(f'Model saved to ==> {save_path}')


def load_metrics(load_path):

    if load_path==None:
        return
    
    state_dict = torch.load(load_path, map_location=device)
    print(f'Model loaded from <== {load_path}')
    
    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']

#这个是词向量矩阵的名字
weights_matrix = torch.Tensor(embeddings_matrix)
#print(weights_matrix)


hidden_size = 128 # 隐藏神经元数量
num_layers = 1 # 搭1层
num_epochs = 20 #训练20次
learning_rate = 1e-3 

#初始化模型
lstm_model = LSTM_MODEL(weights_matrix,hidden_size, num_layers)
lstm_model.train()

# Training Function

def train(model,optimizer,criterion = nn.CrossEntropyLoss(), train_loader = train_loader, valid_loader = val_loader,
                                             num_epochs = 10,
                                             eval_every = len(train_loader) // 2,
                                             best_valid_loss = float("Inf")
                                             ):
      
    # initialize running values
    running_loss = 0.0
    running_acc = 0.0
    valid_running_loss = 0.0
    valid_running_acc = 0.0
    global_step = 0
    train_loss_list = []
    valid_loss_list = []
    global_steps_list = []

    # training loop
    model.train()
    for epoch in range(num_epochs):
        for i, (trains, labels) in enumerate(train_loader):   
            labels = labels.to(device)
            trains = trains.to(device)
            output = model(trains)

            loss = criterion(output, labels)
            pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
            accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # update running values
            running_loss += loss.item()
            running_acc += accuracy
            global_step += 1

            # evaluation step
            if global_step % eval_every == 0:
                model.eval()
                with torch.no_grad():                    
                  # validation loop
                  for i, (vals, labels) in enumerate(valid_loader):
                      labels = labels.to(device)
                      vals = vals.to(device)
                      output = model(vals)
                      pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
                      accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))
                      loss = criterion(output, labels)
                      valid_running_loss += loss.item()
                      valid_running_acc += accuracy

                # evaluation
                average_train_loss = running_loss / eval_every
                average_train_acc = running_acc / eval_every
                average_valid_loss = valid_running_loss / len(valid_loader)
                average_valid_acc = valid_running_acc / len(valid_loader)
                train_loss_list.append(average_train_loss)
                valid_loss_list.append(average_valid_loss)
                global_steps_list.append(global_step)

                # resetting running values
                running_loss = 0.0     
                running_acc = 0.0           
                valid_running_loss = 0.0
                valid_running_acc = 0.0
                model.train()

                # print progress
                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Valid Loss: {:.4f}, Valid Acc: {:.4f}'
                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),
                              average_train_loss, average_train_acc, average_valid_loss,average_valid_acc))
    
    save_metrics('/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    print('Finished Training!')

lstm_model = lstm_model.to(device)
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)
train(model=lstm_model, optimizer=optimizer, num_epochs=20)

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

train_loss_list, valid_loss_list, global_steps_list = load_metrics('/metrics.pt')
plt.plot(global_steps_list, train_loss_list, label='Train')
plt.plot(global_steps_list, valid_loss_list, label='Valid')
plt.xlabel('Global Steps')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
def evaluate(model, test_loader, version='title', threshold=0.5):


    y_pred = []
    y_true = []

    model.eval()
    with torch.no_grad():
        for i, (trains, labels) in enumerate(test_loader):          
            #labels = labels.to(device)
            labels=np.asarray(labels.cpu())
            #print(labels)
            trains = trains.to(device)
            output = model(trains)
            #output = (output > threshold).int()
            output=np.asarray(output.cpu())
            output=np.argmax(output,1)
            #print(output)
            y_pred.extend(output.tolist())
            y_true.extend(labels.tolist())
    print('Classification Report:')
    print(classification_report(y_true, y_pred, target_names=['词','诗','文言文','新闻','期刊'], digits=4))
#     %matplotlib inline
    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2,3,4])
    ax= plt.subplot()
    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt="d")

    ax.set_title('Confusion Matrix')
    ax.set_xlabel('Predicted Labels')
    ax.set_ylabel('True Labels')

    ax.xaxis.set_ticklabels(['ci','poem','wyw','news','journal'])
    ax.yaxis.set_ticklabels(['ci','poem','wyw','news','journal'])

evaluate(lstm_model, test_loader)

class BLSTM_MODEL(nn.Module):      #双向LSTM
    def __init__(self, weights_matrix, hidden_size, num_layers):
        super(BLSTM_MODEL,self).__init__()
        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True) #启用bidirectional参数
        self.drop = nn.Dropout(p=0.25)
        self.fc = nn.Linear(2*hidden_size, 5) #双向LSTM的输出是隐藏神经元数量乘以2

    def forward(self, x):        
        out = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 100, 300]
        #print(out.shape)
        out, _ = self.lstm(out)
        #print(out.shape)
        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state
        #print(out.shape)
        out = F.softmax(out, dim=1)
        return out
    
    def init_hidden(self, batch_size):
        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))

#这个是词向量矩阵的名字
weights_matrix = torch.Tensor(embeddings_matrix)
#print(weights_matrix)


hidden_size = 128 # 隐藏神经元数量
num_layers = 1 # 搭1层
num_epochs = 20 #训练20次
learning_rate = 1e-3 

#初始化双向LSTM模型
lstm_model = BLSTM_MODEL(weights_matrix,hidden_size, num_layers)
lstm_model.train()

# Training Function

def train(model,optimizer,criterion = nn.CrossEntropyLoss(), train_loader = train_loader, valid_loader = val_loader,
                                             num_epochs = 10,
                                             eval_every = len(train_loader) // 2,
                                             best_valid_loss = float("Inf")
                                             ):
      
    # initialize running values
    running_loss = 0.0
    running_acc = 0.0
    valid_running_loss = 0.0
    valid_running_acc = 0.0
    global_step = 0
    train_loss_list = []
    valid_loss_list = []
    global_steps_list = []

    # training loop
    model.train()
    for epoch in range(num_epochs):
        for i, (trains, labels) in enumerate(train_loader):   
            labels = labels.to(device)
            trains = trains.to(device)
            output = model(trains)

            loss = criterion(output, labels)
            pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
            accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # update running values
            running_loss += loss.item()
            running_acc += accuracy
            global_step += 1

            # evaluation step
            if global_step % eval_every == 0:
                model.eval()
                with torch.no_grad():                    
                  # validation loop
                  for i, (vals, labels) in enumerate(valid_loader):
                      labels = labels.to(device)
                      vals = vals.to(device)
                      output = model(vals)
                      pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
                      accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))
                      loss = criterion(output, labels)
                      valid_running_loss += loss.item()
                      valid_running_acc += accuracy

                # evaluation
                average_train_loss = running_loss / eval_every
                average_train_acc = running_acc / eval_every
                average_valid_loss = valid_running_loss / len(valid_loader)
                average_valid_acc = valid_running_acc / len(valid_loader)
                train_loss_list.append(average_train_loss)
                valid_loss_list.append(average_valid_loss)
                global_steps_list.append(global_step)

                # resetting running values
                running_loss = 0.0     
                running_acc = 0.0           
                valid_running_loss = 0.0
                valid_running_acc = 0.0
                model.train()

                # print progress
                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Valid Loss: {:.4f}, Valid Acc: {:.4f}'
                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),
                              average_train_loss, average_train_acc, average_valid_loss,average_valid_acc))
    
    #save_metrics('/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    print('Finished Training!')

blstm_model = lstm_model.to(device)
optimizer = torch.optim.Adam(blstm_model.parameters(), lr=0.001)
train(model=blstm_model, optimizer=optimizer, num_epochs=20)

del X_train, X_test, y_train, y_test, X_val, y_val

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
evaluate(blstm_model, test_loader)



class GRU_MODEL(nn.Module): #GRU
    def __init__(self, weights_matrix, hidden_size, num_layers):
        super(GRU_MODEL,self).__init__()
        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)
        self.drop = nn.Dropout(p=0.25)
        self.fc = nn.Linear(hidden_size, 5)

    def forward(self, x):        
        out = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]
        #print(out.shape)
        out, _ = self.gru(out)  
        #print(out.shape)
        out = self.fc(out[:, -1, :])  # 句子最后时刻的 hidden state
        #print(out.shape)
        out = F.softmax(out, dim=1)
        return out
    
    def init_hidden(self, batch_size):
        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))

#这个是词向量矩阵的名字
weights_matrix = torch.Tensor(embeddings_matrix)
#print(weights_matrix)


hidden_size = 128 # 隐藏神经元数量
num_layers = 1 # 搭1层
num_epochs = 20 #训练20次
learning_rate = 1e-3 

#初始化GRU模型
gru_model = GRU_MODEL(weights_matrix,hidden_size, num_layers)
gru_model.train()


def train(model,optimizer,criterion = nn.CrossEntropyLoss(), train_loader = train_loader, valid_loader = val_loader,
                                             num_epochs = 10,
                                             eval_every = len(train_loader) // 2,
                                             best_valid_loss = float("Inf")
                                             ):
      
    # initialize running values
    running_loss = 0.0
    running_acc = 0.0
    valid_running_loss = 0.0
    valid_running_acc = 0.0
    global_step = 0
    train_loss_list = []
    valid_loss_list = []
    global_steps_list = []

    # training loop
    model.train()
    for epoch in range(num_epochs):
        for i, (trains, labels) in enumerate(train_loader):   
            labels = labels.to(device)
            trains = trains.to(device)
            output = model(trains)

            loss = criterion(output, labels)
            pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
            accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # update running values
            running_loss += loss.item()
            running_acc += accuracy
            global_step += 1

            # evaluation step
            if global_step % eval_every == 0:
                model.eval()
                with torch.no_grad():                    
                  # validation loop
                  for i, (vals, labels) in enumerate(valid_loader):
                      labels = labels.to(device)
                      vals = vals.to(device)
                      output = model(vals)
                      pred_y = torch.max(output, dim=1)[1].data.cpu().numpy()
                      accuracy = ((pred_y == labels.data.cpu().numpy()).astype(int).sum()) / float(labels.size(0))
                      loss = criterion(output, labels)
                      valid_running_loss += loss.item()
                      valid_running_acc += accuracy

                # evaluation
                average_train_loss = running_loss / eval_every
                average_train_acc = running_acc / eval_every
                average_valid_loss = valid_running_loss / len(valid_loader)
                average_valid_acc = valid_running_acc / len(valid_loader)
                train_loss_list.append(average_train_loss)
                valid_loss_list.append(average_valid_loss)
                global_steps_list.append(global_step)

                # resetting running values
                running_loss = 0.0     
                running_acc = 0.0           
                valid_running_loss = 0.0
                valid_running_acc = 0.0
                model.train()

                # print progress
                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Valid Loss: {:.4f}, Valid Acc: {:.4f}'
                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),
                              average_train_loss, average_train_acc, average_valid_loss,average_valid_acc))
                
                # checkpoint
                if best_valid_loss > average_valid_loss:
                    best_valid_loss = average_valid_loss
                    #save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)
                    #save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    
    #save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
    print('Finished Training!')

gru_model = gru_model.to(device)
optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.001)
train(model=gru_model, optimizer=optimizer, num_epochs=20)

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

evaluate(gru_model, test_loader)